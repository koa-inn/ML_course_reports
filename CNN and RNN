Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) are two frameworks built upon the structures and ideas of a typical fully connected neural network (ANN) to make them more suited for and capable of handling more complex data structures such as text sequences and images. Since an ANN can only take a 1-d vector in as an input, trying to feed a 2-d image presents several issues such as feature count explosion as the image gets larger, and loss of important spacial information when flattening. CNN structure offers a powerful framework for solving both of these issues and even allows for advanced feature extraction as convolution layers will automatically learn which parts and aspects of the image are most important for that that model is targeting. These advances, allow for better and less computationally expensive models allowing for a whole host of image related tasks such as image classification, image segmentation, and anomaly detection. These power modern advancements, which would not be possible using a normal ANN, such as autonomous vehicles, real time object detection and classification, and face recognition. As CNNs are to image data, RNNs are to natural language and text data processing. While tokenized word sequences could be fed into a normal ANN, the order of this sequence will be lost, which in the context of language, is a huge drawback. RNNs provide a structure which allows the order of the input to be capture by allowing the each word in the sequence to influenced by the previous words in this sequence. This allows the network to capture the context of a word in the larger context of the sentence allowing much better interpretation of language and other order dependent variables leading to advances such as speech recognition, accurate sentiment analysis, and next word prediction. Both CNNs and RNNs act as add ons to traditional ANNs which allows much better interpretation and performance when using the more complex data input types of images and language.
