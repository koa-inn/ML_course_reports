Clustering is a unique and interesting part of the broader machine learning landscape. Being an unsupervised method means that the data that we are using does not have a known target that the model we are ‘training’ is able to compare its progress against. The nebulous nature of this process leads to much more varied interpretations of what a good clustering solution on a particular set of data might be compared other machine learning methods. In supervised learning methods, typically you essentially have the answer key to what your model should be returning which you can directly compare against. However with clustering, while there are metrics for assessing the ‘quality’ of a clustering output such as the Silhouette Score, one set of data could have many viable clustering solutions. This could depend on the methods used, the goal/target of the clustering to begin with, and even the machine learning engineer’s discretion. Certain methods differ in there approaches and can be better or worst at effective clustering depending on the dataset. Some methods such as DBScan may be able to capture more complex structures such as circular or moon shaped clusters yet struggle with clusters with higher degrees of overlap, while an algorithm like K-Means would perform the vise versa in these scenarios. Additionally, depending on what the end goal of the analysis is, could lead to different optimal solutions. For example, the resulting solution for a company wanting to create segment their customer base into specifically 5 different groups for marketing purposes would likely be much different from an exploratory clustering analysis to try to create features for a customer churn model. While the underlying data set may be the same the goals are different, and this may lead to two different but equally viable clustering solutions. At the end of the day, there is also a lot of discretion in the hands of the one designing the model, choosing how to preprocess the data, or whether the K-Means solution with 4 clusters seems to work better than the solution with 5 clusters. While there may be clustering solutions which are clearly bad, the solution which may be best might not be so clear.
