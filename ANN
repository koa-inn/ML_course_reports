The advent of Artificial Neural Networks (ANNs) provided data scientists with a powerful framework for creating models which could learn and retain complex structures and interactions in data. Following an iterative learning process of slowly adjusting the weights of each node, based on the back-propagation of error combined with a highly customizable and expandable structure, allows these networks to capture more complexity and feature interaction than other regression and classification models. While the base models themselves definitely advanced machine learning capabilities by themselves, the more dramatic progress came from what was built on top of the foundation provided by ANNs. For example, since basic ANNs can only take a vector in as an input, they are not great at interpreting matrix shaped images, but this is where Convolutional Neural Networks (CNNs) come in. Through a combination of convolution layers which apply many filters and pooling layers which compress and reduce the feature space, allow for images to be fed directly into the network and meaningful information to be extracted which is then fed into our typical ANN layers. CNNs can perform a ton of powerful and useful tasks surrounding images such as image object classification and detection or image segmentation, but these allow for real world functionality such as handwriting recognition, face detection, or quality control on a production line. Similarly, Recurrent Neural Networks (RNNs) allow for the network to have stored context from previous inputs allowing for much better suited than base ANNs for tasks involving language and speech as well as time and order dependent tasks such as stock price forecasting. While ANNs themselves are a powerful tool for some machine learning applications, it is the foundation that ANNs provided for more complex and powerful models to be built upon, that have allowed Neural Networks to become a core part of a data scientistâ€™s toolkit.
