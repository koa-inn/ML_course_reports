While modern regression techniques and computational power provide extremely powerful tools for solving machine learning problems, even the best algorithms will fail to accurately describe the data if the model is set up poorly. These articles describe the how regression can be used to help solve practical problems, but they also caution the reader to ensure thoughtfulness when formulating the model to avoid many potential pitfalls. While it is tempting to try to create the most complex and intricate model to describe a given dataset, often times selecting more features, or trying to add in interactions between them can result is less accurate predictions. The articles suggest feature selection methods such as the use of certain statistics such as adjusted R-squared or by analyzing correlation heat maps of the features and targets to determine which features may or may not so a good job of describing the target. Another key contributor towards a modelâ€™s success will be the choices and tuning of hyperparameters. Most model types will have attributes, related to how the model itself functions and is unrelated to the data itself, called hyperparameters. Choosing proper values for hyperparameters could mean the difference between a model which hardly improves the predictions over random guesses to a highly accurate model which does an excellent job in prediction. As such, tuning hyperparameters is highly important for creating an ultimately successful model.
